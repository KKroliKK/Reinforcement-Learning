{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "fi26BiYNajAu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "\n",
        "Implement VGG16, for that write specific `nn.Module`, `VGGBlock` implementing block of VGG."
      ],
      "metadata": {
        "id": "od-IHzGCC2sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ITvQtQzDC1MN"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes: int = 1000, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            self.conv_block(3, 64, 2),\n",
        "            self.conv_block(64, 128, 2),\n",
        "            self.conv_block(128, 256, 3),\n",
        "            self.conv_block(256, 512, 3),\n",
        "            self.conv_block(512, 512, 3)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "    def conv_block(self, in_channels, out_channels, num_convs=2):\n",
        "\n",
        "        conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU()\n",
        "        convs = [nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU()] * (num_convs - 1)\n",
        "        convs.insert(0, conv1)\n",
        "        convs.append(nn.MaxPool2d(2, 2))\n",
        "\n",
        "        return nn.Sequential(*convs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf)\n",
        "\n",
        "## Inception module\n",
        "\n",
        "Write specific `nn.Module` for Inception module."
      ],
      "metadata": {
        "id": "Gi43r1dPDRp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    # `c1`--`c4` are the number of output channels for each branch\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super(Inception, self).__init__(**kwargs)\n",
        "        # Branch 1\n",
        "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
        "        # Branch 2\n",
        "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
        "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
        "        # Branch 3\n",
        "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
        "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
        "        # Branch 4\n",
        "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = F.relu(self.b1_1(x))\n",
        "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
        "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
        "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
        "        return torch.cat((b1, b2, b3, b4), dim=1)"
      ],
      "metadata": {
        "id": "L91nFCXhDhxD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stem network\n",
        "\n",
        "Write down, why do we need a Stem network."
      ],
      "metadata": {
        "id": "Pot5itHXDisN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this subnet is to quickly and strongly reduce the spatial dimensions (compress the image before parallel processing) in order to minimize the number of elements in the layers."
      ],
      "metadata": {
        "id": "nhADh2IVDojD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "Implement ResNet-18, for that write specific `nn.Module`, `ResNetBlock` implementing block of ResNet."
      ],
      "metadata": {
        "id": "uD7dsr2yDuGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        # self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1())\n",
        "        for i, b in enumerate(arch):\n",
        "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        # self.net.apply(d2l.init_cnn)\n",
        "\n",
        "\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        \n",
        "\n",
        "    def block(self, num_residuals, num_channels, first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(Residual(num_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ResNet18().layer_summary((1, 1, 96, 96))\n",
        "ResNet18()"
      ],
      "metadata": {
        "id": "FJxA2J7OEPxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb6f476-a53c-4dde-bb93-ca303dddee04"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18(\n",
              "  (net): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): LazyConv2d(0, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (b2): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (b3): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): LazyConv2d(0, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (b4): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (b5): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): LazyConv2d(0, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (last): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (1): Flatten(start_dim=1, end_dim=-1)\n",
              "      (2): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is kind of strange that `forward()` method is correct but there is no `ReLU` in the printing."
      ],
      "metadata": {
        "id": "YE9eZzEUByus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18()\n",
        "model(torch.ones(1, 3, 30, 50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfHX7GQBArC9",
        "outputId": "dd75ece9-b2ce-485a-e2d9-e843b5db5c3c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3636, -0.1795, -0.9419,  0.0360,  0.2961,  0.3861, -0.4312, -0.1350,\n",
              "          0.5935,  0.7133]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `ResNeXtBlock` implementing block of ResNeXt."
      ],
      "metadata": {
        "id": "cAkVh_wLES6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNeXtBlock(nn.Module):\n",
        "    \"\"\"The ResNeXt block.\"\"\"\n",
        "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
        "                 strides=1):\n",
        "        super().__init__()\n",
        "        bot_channels = int(round(num_channels * bot_mul))\n",
        "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
        "                                   stride=strides, padding=1,\n",
        "                                   groups=bot_channels//groups)\n",
        "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "        self.bn3 = nn.LazyBatchNorm2d()\n",
        "        if use_1x1conv:\n",
        "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "            self.bn4 = nn.LazyBatchNorm2d()\n",
        "        else:\n",
        "            self.conv4 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "        if self.conv4:\n",
        "            X = self.bn4(self.conv4(X))\n",
        "        return F.relu(Y + X)"
      ],
      "metadata": {
        "id": "6FN7E2GADnTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [SENet](https://arxiv.org/pdf/1709.01507.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `SEBlock` implementing block of SENet."
      ],
      "metadata": {
        "id": "ebgJY9YcEw2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SE_Block(nn.Module):\n",
        "    def __init__(self, c, r=16):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(c, c // r, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(c // r, c, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, _, _ = x.shape\n",
        "        y = self.squeeze(x).view(bs, c)\n",
        "        y = self.excitation(y).view(bs, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "BfddhrMkE5un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Neural Architecture Search](https://arxiv.org/pdf/1611.01578.pdf)\n",
        "\n",
        "For the neural network of your assignment 2, write down the parametrization of the network you would use for the NAS."
      ],
      "metadata": {
        "id": "px-Os_viE-aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs_size = ...\n",
        "HIDDEN_SIZE = ...\n",
        "n_actions = ...\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(obs_size, HIDDEN_SIZE),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(HIDDEN_SIZE, n_actions),\n",
        "            torch.nn.Softmax(dim=0)\n",
        "     )"
      ],
      "metadata": {
        "id": "psigwreg2CnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of hidden layers\n",
        "\n",
        "Hdden size\n",
        "\n",
        "Activation functions"
      ],
      "metadata": {
        "id": "sojINj1YFeMU"
      }
    }
  ]
}